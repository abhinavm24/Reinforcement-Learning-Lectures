{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL Lecture 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhawvipul/Reinforcement-Learning-Lectures/blob/master/RL_Lecture_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wamOdwXNYt1h",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning - Lecture 2\n",
        "\n",
        "We will be discussing Model based RL, Markov decision processes and exploration vs exploitation dilemma.\n",
        "\n",
        "![Taxonomy of RL algorithms](https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg)\n",
        "\n",
        "[citations for the images](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#citations-below)\n",
        "\n",
        "The image above is non exhaustive but useful representation of RL algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "## Policy Search\n",
        "\n",
        "* Number of deterministic policies - |A|<sup>|S|</sup> \n",
        "* Policy iteration is generally more efficient than enumeration.\n",
        "\n",
        "* Note to self - Talk about **Policy Iteration** algorithm.\n",
        "  * State Action Value(Q)\n",
        "  * Policy iteration algorithm\n",
        "  * Policy improvement\n",
        "  * Monotonic improvement\n",
        "  \n",
        "* Policy Iteration computes the optimal policy\n",
        "* Another approach is **value iteration**\n",
        "\n",
        "\n",
        "## Bellman Equation and BV operator\n",
        "\n",
        "## Value Iteration\n",
        "\n",
        "## Proof that Bellman Backup is a contraction on V for gamma<1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2kSo0APyEx",
        "colab_type": "text"
      },
      "source": [
        "## Two fundamental problems in RL\n",
        "\n",
        "\n",
        "## Exoploration vs Exploitation dilemma\n",
        "\n",
        "* Multiarm bandit\n",
        "* Regret \n",
        "* epsilon-greedy algorithm\n",
        "* Algorithm idea as an assignment\n",
        "* UCB\n",
        "* Read about Hoeffding's inequality\n",
        "* Bayesian Bandits\n",
        "* Probability Matching\n"
      ]
    }
  ]
}