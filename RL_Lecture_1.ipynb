{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL Lecture 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhawvipul/Reinforcement-Learning-Lectures/blob/master/RL_Lecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2Ik9yrtpmX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Reinforcement Learning\n",
        "\n",
        "**Learn** to make **good sequence of decisions**! \n",
        "\n",
        "The fundamental challenge in machine learning is learning to make good decisions under uncertainity. \n",
        "\n",
        "---\n",
        "\n",
        "## Types of Learning - \n",
        "\n",
        "*   Active Learning\n",
        "*   Passive Learning\n",
        "\n",
        "A human can learn **without examples** of optimal behaviour. Right?\n",
        "\n",
        "## Why Learn?\n",
        "\n",
        "There are two distinct reasons to learn - \n",
        "\n",
        "1.   FInd previously unknown solutions\n",
        "2.   Find solutions **online** in case of unforeseen circumstances. (Not only generalization but learning online)\n",
        "\n",
        "RL seeks to provide algorithm for both the cases.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Reinforcement Learning?\n",
        "\n",
        "RL is a science of making decisions from interaction. Is RL=AI?\n",
        "\n",
        "## How is RL different from Machine Learning ?\n",
        "\n",
        "Reinforcement Learning is different from other machine learning paradigms because - \n",
        "\n",
        "*   No Supervision, only **reward** signal.\n",
        "*   Delayed feedback\n",
        "*   Previous decisions might affect future predictions/interactions.\n",
        "\n",
        "There is a fine line between imitation learning and reinforcement learning. Can you guess what?\n",
        "\n",
        "## What does RL involves?\n",
        "\n",
        "* Optimization - the optimal way to make decisions which yields best outcomes.\n",
        "* Delayed consequences - decisions made now can \n",
        "* Exploration \n",
        "* Generalization\n",
        "\n",
        "\n",
        "## Core Concepts of Reinforcement Learning - \n",
        "\n",
        "* Environment\n",
        "* Reward Signal\n",
        "* Agent - \n",
        "    *   Agent State\n",
        "    *   Policy\n",
        "    *   Value Function (probable)\n",
        "    *   Model (Optional)\n",
        "\n",
        "![Agent Environment Interaction](https://github.com/vaibhawvipul/Reinforcement-Learning-Lectures/blob/master/agent-env-rl.jpeg?raw=true)\n",
        "\n",
        "---\n",
        "\n",
        "## Behaviour and Intelligence - \n",
        "\n",
        "Let's spend some time in understanding this interesting creature - **Sea Squirt**\n",
        "\n",
        "![Sea Squirt](https://goodheartextremescience.files.wordpress.com/2010/01/sea_squirts_img_0704.jpg)\n",
        "\n",
        "Sea Squirts are a primitive creature most famous for “eating their brains.”  \n",
        "\n",
        "Sea squirts are hermaphrodites—they have both male and female reproductive organs. They reproduce by releasing eggs and sperm into the water at the same time. When eggs develop into tadpole-like larvae, they swin by wiggle and twich movements.\n",
        "\n",
        "The free-swimming larvae stage lasts only a short time, since the larvae aren’t capable of feeding. Soon, they settle to either the bottom of sea floor or on some rock and they cement themselves headfirst to the spot where they will spend the rest of their lives. \n",
        "\n",
        "The sea squirt larvae begin absorbing all the tadpole-like parts. Where the sea squirt larva once had gills, it develops the intake and exist siphons that will help it bring water and food into its body. It absorbs its twitching tail. It absorbs its primitive eye and its spine-like notocord. Finally, it even absorbs the rudimentary little “brain” (cerebral ganglion) that it used to swim about and find its attachment place.\n",
        "\n",
        "Since the sea squirt no longer needs its brain to help it swim around or to see, this isn’t a great loss to the creature.  Read more about this fascinatin creature [here](https://goodheartextremescience.wordpress.com/2010/01/27/meet-the-creature-that-eats-its-own-brain/).\n",
        "\n",
        "This example above suggests that brain is helping in decision making, so no more decisions needed then no need for brain?\n",
        "\n",
        "This helps us in reminding **why an agent needs to be intelligent, it is because it has to make decisions!**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA20zvGMHQAG",
        "colab_type": "text"
      },
      "source": [
        "Let us install **OpenAI Gym**. We will be using OpenAI's Gym for a lot of tutorials and exercises. \n",
        "\n",
        "OpenAI Gym - Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\n",
        "\n",
        "Here is how you can install openAI Gym.\n",
        "\n",
        "```\n",
        "pip install gym\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Following is a small demo of OpenAI Gym - \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It7QCOwjIMBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DJhGgsKIOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89jspzrtJrA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the necessary modules and dependencies\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZWdhRwKpaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVxHLA7lKwER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1MsGoc1K7Lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceiguS8MLBTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Zb5hndLEje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBrpak0JLnZ4",
        "colab_type": "text"
      },
      "source": [
        "The above code is also an example of how to run openai's gym in colab! Please make sure that you note is down somewhere.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLFELKrfMMea",
        "colab_type": "text"
      },
      "source": [
        "## Agent and Environment\n",
        "\n",
        "At each time step t the agent :\n",
        "* receives an observation or environment state and reward.\n",
        "* executes an Action\n",
        "\n",
        "The environment:\n",
        "* receives an action\n",
        "* emits observation and reward\n",
        "\n",
        "---\n",
        "\n",
        "## Rewards\n",
        "\n",
        "A reward is a scalar feedback(real-valued) signal. It indicates how well an agent is doing at the time step t. The agent's job is to maximize the *cummulative reward* (called **return**).\n",
        "\n",
        "**Reward Hypothesis** - Any goal can be formalized as the outcome of maximizing cummulative rewards.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMaXcAscQK-V",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## History\n",
        "\n",
        "A history is a sequence of past observations, actions and rewards. Agent chooses an action based on history\n",
        "\n",
        "This history can then be used to construct **agent state**.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVllZ1EoQ5HR",
        "colab_type": "text"
      },
      "source": [
        "## Value\n",
        "\n",
        "The expected cummulative reward from state S is called Value.\n",
        "\n",
        "The goal is then to maximize value by choosing suitable actions. \n",
        "\n",
        "Note - Returns and Values can be defined recursively. \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydKhWgMKTBsw",
        "colab_type": "text"
      },
      "source": [
        "## Policy \n",
        "\n",
        "Mapping from states to actions is called policy.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh5pGYAoTMZr",
        "colab_type": "text"
      },
      "source": [
        "## Markov Assumption\n",
        "\n",
        "The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model.\n",
        "\n",
        "A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\n",
        "\n",
        "Future is independent of the past given the present.\n",
        "\n",
        "## Fully Observable Environments \n",
        "\n",
        "Suppose agent sees full environment state then the agent state is equal to observed state of the environment(e.g Board game states).\n",
        "\n",
        "Let the current blood pressure be the current state. The action is whether to take medication or not is that markov?\n",
        "\n",
        "## Why is Markov so popular?\n",
        "\n",
        "* It can always be satisfied.\n",
        "* In practice, often assume that the recent state is the sufficient statistic of history. However, this notion is changing with deep learning with the arrival of lstms etc.\n",
        "\n",
        "## Partially Observable MDP (POMDP)\n",
        "\n",
        "* Agent State is not same as the world state.\n",
        "* The environment state can still be markov but the agent doesn't know it. \n",
        "\n",
        "example - Poker.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AWPLW1kZnw7",
        "colab_type": "text"
      },
      "source": [
        "## RL algorithm components - \n",
        "\n",
        "Often includes one or more of - \n",
        "\n",
        "* Model - How the world changes in response to the agent's action.\n",
        "* Policy - Mapping from agent state to actions.\n",
        "* Value Function - Future reward from being in a state and action following a particular policy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Types of RL agents \n",
        "\n",
        "* Model based\n",
        "* Model free\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4P57ExcajZQ",
        "colab_type": "text"
      },
      "source": [
        "## Value Function \n",
        "\n",
        "Expected discounted sum of future rewards under a particular policy. It can be used to compare policies. \n",
        "\n",
        "**Bellman Equation** "
      ]
    }
  ]
}